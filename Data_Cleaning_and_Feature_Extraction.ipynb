{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94657ec",
   "metadata": {},
   "source": [
    "# Phase 2 â€” Data Cleaning & Feature Extraction\n",
    "\n",
    "This notebook performs:\n",
    "1) Data cleaning / preprocessing of raw audio clips\n",
    "2) Optional augmentation (controlled)\n",
    "3) Feature extraction to fixed-size numeric vectors (for later ML)\n",
    "\n",
    "Dataset structure (expected):\n",
    "dataset/\n",
    "  German/ Male/*.wav, Female/*.wav\n",
    "  Italian/ Male/*.wav, Female/*.wav\n",
    "  Korean/ Male/*.wav, Female/*.wav\n",
    "  Spanish/ Male/*.wav, Female/*.wav\n",
    "\n",
    "Outputs:\n",
    "- artifacts/metadata.csv\n",
    "- artifacts/features.csv (or .parquet if you prefer)\n",
    "- artifacts/X.npy, artifacts/y.npy\n",
    "- (optional) processed_audio/ cleaned wav files\n",
    "- (optional) processed_audio_aug/ augmented wav files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbefea0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- `DATASET_DIR`: path to your `dataset` folder\n",
    "- `SR`: target sample rate for all audio\n",
    "- `TARGET_SECONDS`: pad/trim each clip to exactly this length\n",
    "- `SAVE_CLEANED_AUDIO`: if True, write cleaned audio to disk (recommended for reproducibility)\n",
    "- `USE_AUGMENTATION`: if True, create augmented variants (saved to disk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"dataset\")\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "CLEAN_DIR = Path(\"processed_audio\")\n",
    "AUG_DIR = Path(\"processed_audio_aug\")\n",
    "\n",
    "SR = 16000\n",
    "TARGET_SECONDS = 60.0\n",
    "TARGET_SAMPLES = int(SR * TARGET_SECONDS)\n",
    "\n",
    "TRIM_SILENCE = True\n",
    "TOP_DB = 25\n",
    "\n",
    "RMS_TARGET = 0.1\n",
    "PEAK_MAX = 0.99\n",
    "\n",
    "SAVE_CLEANED_AUDIO = True\n",
    "\n",
    "USE_AUGMENTATION = False\n",
    "AUG_COPIES_PER_FILE = 1\n",
    "\n",
    "RNG_SEED = 42\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if SAVE_CLEANED_AUDIO:\n",
    "    CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if USE_AUGMENTATION:\n",
    "    AUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LANGUAGES = [\"German\", \"Italian\", \"Korean\", \"Spanish\"]\n",
    "GENDERS = [\"Male\", \"Female\"]\n",
    "AUDIO_EXTS = [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea301ba3",
   "metadata": {},
   "source": [
    "## 1) Index the dataset\n",
    "\n",
    "We scan the folder tree and build a table with:\n",
    "- file path\n",
    "- language label\n",
    "- gender label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac22e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                path language  gender\n",
       " 0  dataset\\Italian\\Female\\810104250_female_italia...  Italian  Female\n",
       " 1  dataset\\Italian\\Female\\810101502_female_italia...  Italian  Female\n",
       " 2  dataset\\German\\Male\\810103040_male_german_voic...   German    Male\n",
       " 3  dataset\\Italian\\Male\\810101441_male_italian_vo...  Italian    Male\n",
       " 4  dataset\\Korean\\Female\\810100094_female_korean_...   Korean  Female,\n",
       " (720, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_audio_files(dataset_dir: Path):\n",
    "    rows = []\n",
    "    for lang in LANGUAGES:\n",
    "        for gender in GENDERS:\n",
    "            base = dataset_dir / lang / gender\n",
    "            if not base.exists():\n",
    "                continue\n",
    "            for ext in AUDIO_EXTS:\n",
    "                for fp in glob.glob(str(base / f\"**/*.{ext}\"), recursive=True):\n",
    "                    rows.append({\"path\": str(Path(fp)), \"language\": lang, \"gender\": gender})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = list_audio_files(DATASET_DIR)\n",
    "df = df.sample(frac=1.0, random_state=RNG_SEED).reset_index(drop=True)\n",
    "\n",
    "df.head(), df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa4d9f",
   "metadata": {},
   "source": [
    "## 2) Audio loading + cleaning utilities\n",
    "\n",
    "Cleaning steps:\n",
    "- mono + resample to SR\n",
    "- optional trim silence\n",
    "- remove DC offset\n",
    "- RMS normalize + peak guard\n",
    "- pad/trim to TARGET_SECONDS\n",
    "- quality checks (nan/inf, all-zero, too-short, clipping ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11db5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_audio(path, sr=SR):\n",
    "    try:\n",
    "        y, _ = librosa.load(path, sr=sr, mono=True)\n",
    "        if y is None:\n",
    "            return None, \"load_failed\"\n",
    "        if len(y) == 0:\n",
    "            return None, \"empty_audio\"\n",
    "        if not np.isfinite(y).all():\n",
    "            return None, \"non_finite\"\n",
    "        return y.astype(np.float32), None\n",
    "    except Exception:\n",
    "        return None, \"load_exception\"\n",
    "\n",
    "def trim_silence(y, top_db=TOP_DB):\n",
    "    if len(y) == 0:\n",
    "        return y\n",
    "    yt, _ = librosa.effects.trim(y, top_db=top_db)\n",
    "    return yt.astype(np.float32)\n",
    "\n",
    "def rms_normalize(y, target_rms=RMS_TARGET):\n",
    "    rms = float(np.sqrt(np.mean(y**2)) + 1e-12)\n",
    "    gain = target_rms / rms\n",
    "    return (y * gain).astype(np.float32)\n",
    "\n",
    "def peak_guard(y, peak_max=PEAK_MAX):\n",
    "    peak = float(np.max(np.abs(y)) + 1e-12)\n",
    "    if peak > peak_max:\n",
    "        y = y * (peak_max / peak)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def pad_or_trim(y, target_len=TARGET_SAMPLES):\n",
    "    n = len(y)\n",
    "    if n == target_len:\n",
    "        return y\n",
    "    if n > target_len:\n",
    "        return y[:target_len]\n",
    "    pad = target_len - n\n",
    "    return np.pad(y, (0, pad), mode=\"constant\").astype(np.float32)\n",
    "\n",
    "def clipping_ratio(y, thr=0.99):\n",
    "    return float(np.mean(np.abs(y) >= thr))\n",
    "\n",
    "def clean_audio(y):\n",
    "    y = y - float(np.mean(y))\n",
    "    if TRIM_SILENCE:\n",
    "        y = trim_silence(y, top_db=TOP_DB)\n",
    "    if len(y) < int(0.5 * SR):\n",
    "        return None, \"too_short_after_trim\"\n",
    "    y = rms_normalize(y, target_rms=RMS_TARGET)\n",
    "    y = peak_guard(y, peak_max=PEAK_MAX)\n",
    "    y = pad_or_trim(y, target_len=TARGET_SAMPLES)\n",
    "    if not np.isfinite(y).all():\n",
    "        return None, \"non_finite_after_clean\"\n",
    "    return y, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d9379",
   "metadata": {},
   "source": [
    "## 3) Run cleaning on all files\n",
    "\n",
    "We:\n",
    "- load each file safely\n",
    "- clean it\n",
    "- optionally save cleaned audio\n",
    "- store metadata (duration, clipping ratio, errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29f56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                path  \\\n",
       " 0  dataset\\Italian\\Female\\810104250_female_italia...   \n",
       " 1  dataset\\Italian\\Female\\810101502_female_italia...   \n",
       " 2  dataset\\German\\Male\\810103040_male_german_voic...   \n",
       " 3  dataset\\Italian\\Male\\810101441_male_italian_vo...   \n",
       " 4  dataset\\Korean\\Female\\810100094_female_korean_...   \n",
       " \n",
       "                                           clean_path language  gender status  \\\n",
       " 0  processed_audio\\Italian\\Female\\810104250_femal...  Italian  Female     ok   \n",
       " 1  processed_audio\\Italian\\Female\\810101502_femal...  Italian  Female     ok   \n",
       " 2  processed_audio\\German\\Male\\810103040_male_ger...   German    Male     ok   \n",
       " 3  processed_audio\\Italian\\Male\\810101441_male_it...  Italian    Male     ok   \n",
       " 4  processed_audio\\Korean\\Female\\810100094_female...   Korean  Female     ok   \n",
       " \n",
       "   error  samples  seconds  clipping_ratio  language_id  gender_id  \n",
       " 0         960000     60.0        0.000000            1          0  \n",
       " 1         960000     60.0        0.000000            1          0  \n",
       " 2         960000     60.0        0.000001            0          1  \n",
       " 3         960000     60.0        0.000001            1          1  \n",
       " 4         960000     60.0        0.000000            2          0  ,\n",
       " status\n",
       " ok    720\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_out_path(in_path: str, out_root: Path, lang: str, gender: str):\n",
    "    in_path = Path(in_path)\n",
    "    stem = in_path.stem\n",
    "    out_dir = out_root / lang / gender\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir / f\"{stem}.wav\"\n",
    "\n",
    "meta_rows = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    p = row[\"path\"]\n",
    "    lang = row[\"language\"]\n",
    "    gender = row[\"gender\"]\n",
    "\n",
    "    y, err = safe_load_audio(p, sr=SR)\n",
    "    if err is not None:\n",
    "        meta_rows.append({\n",
    "            \"path\": p, \"language\": lang, \"gender\": gender,\n",
    "            \"status\": \"bad\", \"error\": err, \"samples\": 0,\n",
    "            \"seconds\": 0.0, \"clipping_ratio\": np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    y_clean, err2 = clean_audio(y)\n",
    "    if err2 is not None:\n",
    "        meta_rows.append({\n",
    "            \"path\": p, \"language\": lang, \"gender\": gender,\n",
    "            \"status\": \"bad\", \"error\": err2, \"samples\": len(y),\n",
    "            \"seconds\": float(len(y)/SR), \"clipping_ratio\": clipping_ratio(y)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    out_path = \"\"\n",
    "    if SAVE_CLEANED_AUDIO:\n",
    "        out_fp = make_out_path(p, CLEAN_DIR, lang, gender)\n",
    "        sf.write(out_fp, y_clean, SR)\n",
    "        out_path = str(out_fp)\n",
    "\n",
    "    meta_rows.append({\n",
    "        \"path\": p, \"clean_path\": out_path,\n",
    "        \"language\": lang, \"gender\": gender,\n",
    "        \"status\": \"ok\", \"error\": \"\",\n",
    "        \"samples\": int(len(y_clean)),\n",
    "        \"seconds\": float(len(y_clean)/SR),\n",
    "        \"clipping_ratio\": clipping_ratio(y_clean)\n",
    "    })\n",
    "\n",
    "meta = pd.DataFrame(meta_rows)\n",
    "meta[\"language_id\"] = meta[\"language\"].astype(\"category\").cat.codes\n",
    "meta[\"gender_id\"] = meta[\"gender\"].astype(\"category\").cat.codes\n",
    "\n",
    "meta.head(), meta[\"status\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf71389",
   "metadata": {},
   "source": [
    "### Save metadata + quick cleaning diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddff8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'total_files_indexed': 720,\n",
       "  'ok_files': 720,\n",
       "  'bad_files': 0,\n",
       "  'mean_clipping_ratio_ok': 1.9675925925925927e-07},\n",
       " Series([], Name: count, dtype: int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_path = ARTIFACTS_DIR / \"metadata.csv\"\n",
    "meta.to_csv(meta_path, index=False)\n",
    "\n",
    "ok_meta = meta[meta[\"status\"] == \"ok\"].copy()\n",
    "bad_meta = meta[meta[\"status\"] != \"ok\"].copy()\n",
    "\n",
    "summary = {\n",
    "    \"total_files_indexed\": int(len(meta)),\n",
    "    \"ok_files\": int(len(ok_meta)),\n",
    "    \"bad_files\": int(len(bad_meta)),\n",
    "    \"mean_clipping_ratio_ok\": float(ok_meta[\"clipping_ratio\"].mean()) if len(ok_meta) else np.nan\n",
    "}\n",
    "summary, bad_meta[\"error\"].value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b3a61",
   "metadata": {},
   "source": [
    "## 4) Optional augmentation\n",
    "\n",
    "If enabled, we create `AUG_COPIES_PER_FILE` augmented versions per cleaned clip.\n",
    "Augmentations:\n",
    "- time shift\n",
    "- additive noise\n",
    "- time stretch\n",
    "- pitch shift\n",
    "- gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4424eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_time_shift(y, max_shift_sec=0.5):\n",
    "    max_shift = int(max_shift_sec * SR)\n",
    "    shift = int(rng.integers(-max_shift, max_shift + 1))\n",
    "    return np.roll(y, shift).astype(np.float32)\n",
    "\n",
    "def aug_add_noise(y, snr_db_low=15, snr_db_high=30):\n",
    "    snr_db = float(rng.uniform(snr_db_low, snr_db_high))\n",
    "    sig_power = float(np.mean(y**2) + 1e-12)\n",
    "    noise_power = sig_power / (10 ** (snr_db / 10))\n",
    "    noise = rng.normal(0, math.sqrt(noise_power), size=y.shape).astype(np.float32)\n",
    "    return (y + noise).astype(np.float32)\n",
    "\n",
    "def aug_time_stretch(y, rate_low=0.95, rate_high=1.05):\n",
    "    rate = float(rng.uniform(rate_low, rate_high))\n",
    "    ys = librosa.effects.time_stretch(y, rate=rate).astype(np.float32)\n",
    "    return pad_or_trim(ys, TARGET_SAMPLES)\n",
    "\n",
    "def aug_pitch_shift(y, steps_low=-1.0, steps_high=1.0):\n",
    "    steps = float(rng.uniform(steps_low, steps_high))\n",
    "    yp = librosa.effects.pitch_shift(y, sr=SR, n_steps=steps).astype(np.float32)\n",
    "    return pad_or_trim(yp, TARGET_SAMPLES)\n",
    "\n",
    "def aug_gain(y, db_low=-2.0, db_high=2.0):\n",
    "    db = float(rng.uniform(db_low, db_high))\n",
    "    g = 10 ** (db / 20)\n",
    "    return peak_guard((y * g).astype(np.float32), PEAK_MAX)\n",
    "\n",
    "def apply_random_augmentation(y):\n",
    "    ops = [aug_time_shift, aug_add_noise, aug_time_stretch, aug_pitch_shift, aug_gain]\n",
    "    k = int(rng.integers(1, 4))\n",
    "    chosen = rng.choice(ops, size=k, replace=False)\n",
    "    ya = y.copy()\n",
    "    for op in chosen:\n",
    "        ya = op(ya)\n",
    "    ya = peak_guard(ya, PEAK_MAX)\n",
    "    return ya.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_CONFIG = {\n",
    "    \"time_shift\": {\"enabled\": True, \"max_shift_sec\": 0.5, \"p\": 0.60},\n",
    "    \"add_noise\":  {\"enabled\": True, \"snr_db_low\": 18, \"snr_db_high\": 35, \"p\": 0.70},\n",
    "    \"time_stretch\":{\"enabled\": True, \"rate_low\": 0.97, \"rate_high\": 1.03, \"p\": 0.40},\n",
    "    \"pitch_shift\":{\"enabled\": True, \"steps_low\": -0.5, \"steps_high\": 0.5, \"p\": 0.35},\n",
    "    \"gain\":       {\"enabled\": True, \"db_low\": -1.5, \"db_high\": 1.5, \"p\": 0.60},\n",
    "}\n",
    "\n",
    "AUGMENT_MIN_OPS = 1\n",
    "AUGMENT_MAX_OPS = 3\n",
    "\n",
    "def apply_random_augmentation(y):\n",
    "    ya = y.astype(np.float32).copy()\n",
    "    ops = []\n",
    "\n",
    "    if AUGMENT_CONFIG[\"time_shift\"][\"enabled\"]:\n",
    "        ops.append((\"time_shift\", lambda x: aug_time_shift(x, max_shift_sec=AUGMENT_CONFIG[\"time_shift\"][\"max_shift_sec\"]),\n",
    "                    AUGMENT_CONFIG[\"time_shift\"][\"p\"]))\n",
    "    if AUGMENT_CONFIG[\"add_noise\"][\"enabled\"]:\n",
    "        ops.append((\"add_noise\", lambda x: aug_add_noise(x,\n",
    "                                                        snr_db_low=AUGMENT_CONFIG[\"add_noise\"][\"snr_db_low\"],\n",
    "                                                        snr_db_high=AUGMENT_CONFIG[\"add_noise\"][\"snr_db_high\"]),\n",
    "                    AUGMENT_CONFIG[\"add_noise\"][\"p\"]))\n",
    "    if AUGMENT_CONFIG[\"time_stretch\"][\"enabled\"]:\n",
    "        ops.append((\"time_stretch\", lambda x: aug_time_stretch(x,\n",
    "                                                              rate_low=AUGMENT_CONFIG[\"time_stretch\"][\"rate_low\"],\n",
    "                                                              rate_high=AUGMENT_CONFIG[\"time_stretch\"][\"rate_high\"]),\n",
    "                    AUGMENT_CONFIG[\"time_stretch\"][\"p\"]))\n",
    "    if AUGMENT_CONFIG[\"pitch_shift\"][\"enabled\"]:\n",
    "        ops.append((\"pitch_shift\", lambda x: aug_pitch_shift(x,\n",
    "                                                            steps_low=AUGMENT_CONFIG[\"pitch_shift\"][\"steps_low\"],\n",
    "                                                            steps_high=AUGMENT_CONFIG[\"pitch_shift\"][\"steps_high\"]),\n",
    "                    AUGMENT_CONFIG[\"pitch_shift\"][\"p\"]))\n",
    "    if AUGMENT_CONFIG[\"gain\"][\"enabled\"]:\n",
    "        ops.append((\"gain\", lambda x: aug_gain(x,\n",
    "                                              db_low=AUGMENT_CONFIG[\"gain\"][\"db_low\"],\n",
    "                                              db_high=AUGMENT_CONFIG[\"gain\"][\"db_high\"]),\n",
    "                    AUGMENT_CONFIG[\"gain\"][\"p\"]))\n",
    "\n",
    "    enabled_ops = [(name, fn, p) for (name, fn, p) in ops if p > 0]\n",
    "    if len(enabled_ops) == 0:\n",
    "        return ya\n",
    "\n",
    "    k = int(rng.integers(AUGMENT_MIN_OPS, AUGMENT_MAX_OPS + 1))\n",
    "    rng.shuffle(enabled_ops)\n",
    "\n",
    "    applied = 0\n",
    "    for name, fn, p in enabled_ops:\n",
    "        if applied >= k:\n",
    "            break\n",
    "        if float(rng.random()) <= float(p):\n",
    "            ya = fn(ya)\n",
    "            applied += 1\n",
    "\n",
    "    if applied == 0:\n",
    "        name, fn, _ = enabled_ops[int(rng.integers(0, len(enabled_ops)))]\n",
    "        ya = fn(ya)\n",
    "\n",
    "    ya = peak_guard(ya, PEAK_MAX)\n",
    "    ya = pad_or_trim(ya, TARGET_SAMPLES)\n",
    "    return ya.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75d6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AUG_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m USE_AUGMENTATION = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      2\u001b[39m AUG_COPIES_PER_FILE = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mAUG_DIR\u001b[49m.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m aug_rows = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_AUGMENTATION:\n",
      "\u001b[31mNameError\u001b[39m: name 'AUG_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "USE_AUGMENTATION = False\n",
    "AUG_COPIES_PER_FILE = 1\n",
    "\n",
    "AUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "aug_rows = []\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "    ok_meta2 = ok_meta.copy()\n",
    "    for _, r in ok_meta2.iterrows():\n",
    "        src_path = r[\"clean_path\"] if SAVE_CLEANED_AUDIO and r[\"clean_path\"] else r[\"path\"]\n",
    "        y, err = safe_load_audio(src_path, sr=SR)\n",
    "        if err is not None:\n",
    "            continue\n",
    "        y, err2 = clean_audio(y)\n",
    "        if err2 is not None:\n",
    "            continue\n",
    "\n",
    "        for j in range(AUG_COPIES_PER_FILE):\n",
    "            ya = apply_random_augmentation(y)\n",
    "            out_fp = make_out_path(r[\"path\"], AUG_DIR, r[\"language\"], r[\"gender\"])\n",
    "            out_fp = out_fp.with_name(out_fp.stem + f\"_aug{j+1}.wav\")\n",
    "            sf.write(out_fp, ya, SR)\n",
    "            aug_rows.append({\n",
    "                \"orig_path\": r[\"path\"],\n",
    "                \"aug_path\": str(out_fp),\n",
    "                \"language\": r[\"language\"],\n",
    "                \"gender\": r[\"gender\"]\n",
    "            })\n",
    "\n",
    "aug_df = pd.DataFrame(aug_rows, columns=[\"orig_path\",\"aug_path\",\"language\",\"gender\"])\n",
    "\n",
    "aug_df.head(), aug_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9660eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique augmented languages: language\n",
      "Italian    180\n",
      "German     180\n",
      "Korean     180\n",
      "Spanish    180\n",
      "Name: count, dtype: int64\n",
      "Unique augmented genders: gender\n",
      "Female    360\n",
      "Male      360\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique augmented languages:\", aug_df[\"language\"].value_counts())\n",
    "print(\"Unique augmented genders:\", aug_df[\"gender\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e14309",
   "metadata": {},
   "source": [
    "## 5) Feature extraction\n",
    "\n",
    "We compute:\n",
    "- MFCC (n=20) + delta + delta2\n",
    "- Log-mel spectrogram (n_mels=64)\n",
    "- Spectral descriptors: zcr, rms, centroid, bandwidth, rolloff, contrast\n",
    "\n",
    "Then we aggregate each feature over time frames with:\n",
    "mean, std, min, max, median, skew, kurtosis\n",
    "\n",
    "Result: one fixed-size vector per audio clip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9aa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_1d(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    return np.array([\n",
    "        np.mean(x), np.std(x), np.min(x), np.max(x), np.median(x),\n",
    "        skew(x, bias=False), kurtosis(x, bias=False)\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "def stats_2d(M):\n",
    "    feats = []\n",
    "    for i in range(M.shape[0]):\n",
    "        feats.append(stats_1d(M[i]))\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "def extract_features(y, sr=SR, n_mfcc=20, n_mels=64):\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    d1 = librosa.feature.delta(mfcc)\n",
    "    d2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    logmel = librosa.power_to_db(mel + 1e-12)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.85)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "\n",
    "    feat_parts = [\n",
    "        stats_2d(mfcc), stats_2d(d1), stats_2d(d2),\n",
    "        stats_2d(logmel),\n",
    "        stats_2d(zcr), stats_2d(rms),\n",
    "        stats_2d(centroid), stats_2d(bandwidth), stats_2d(rolloff),\n",
    "        stats_2d(contrast)\n",
    "    ]\n",
    "    return np.concatenate(feat_parts, axis=0).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc66ffe",
   "metadata": {},
   "source": [
    "### Build the feature dataset (cleaned + optional augmented)\n",
    "\n",
    "We extract features from:\n",
    "- cleaned clips (always)\n",
    "- augmented clips (if `USE_AUGMENTATION=True`)\n",
    "\n",
    "We store:\n",
    "- X: features\n",
    "- y: language label\n",
    "- plus a feature table with identifiers (path, language, gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fc670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1440, 952),\n",
       "                                                 path variant language  gender  \\\n",
       " 0  dataset\\Italian\\Female\\810104250_female_italia...   clean  Italian  Female   \n",
       " 1  dataset\\Italian\\Female\\810101502_female_italia...   clean  Italian  Female   \n",
       " 2  dataset\\German\\Male\\810103040_male_german_voic...   clean   German    Male   \n",
       " 3  dataset\\Italian\\Male\\810101441_male_italian_vo...   clean  Italian    Male   \n",
       " 4  dataset\\Korean\\Female\\810100094_female_korean_...   clean   Korean  Female   \n",
       " \n",
       "          y  \n",
       " 0  Italian  \n",
       " 1  Italian  \n",
       " 2   German  \n",
       " 3  Italian  \n",
       " 4   Korean  )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_rows = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "def load_for_features(r):\n",
    "    src = r[\"clean_path\"] if SAVE_CLEANED_AUDIO and isinstance(r.get(\"clean_path\",\"\"), str) and len(r.get(\"clean_path\",\"\")) > 0 else r[\"path\"]\n",
    "    y, err = safe_load_audio(src, sr=SR)\n",
    "    if err is not None:\n",
    "        return None, err\n",
    "    y, err2 = clean_audio(y)\n",
    "    if err2 is not None:\n",
    "        return None, err2\n",
    "    return y, None\n",
    "\n",
    "for _, r in ok_meta.iterrows():\n",
    "    y, e = load_for_features(r)\n",
    "    if e is not None:\n",
    "        continue\n",
    "    feats = extract_features(y, SR)\n",
    "    X_list.append(feats)\n",
    "    y_list.append(r[\"language\"])\n",
    "    feature_rows.append({\n",
    "        \"path\": r[\"path\"],\n",
    "        \"variant\": \"clean\",\n",
    "        \"language\": r[\"language\"],\n",
    "        \"gender\": r[\"gender\"]\n",
    "    })\n",
    "\n",
    "if USE_AUGMENTATION and len(aug_df) > 0:\n",
    "    for _, r in aug_df.iterrows():\n",
    "        y, err = safe_load_audio(r[\"aug_path\"], sr=SR)\n",
    "        if err is not None:\n",
    "            continue\n",
    "        y, err2 = clean_audio(y)\n",
    "        if err2 is not None:\n",
    "            continue\n",
    "        feats = extract_features(y, SR)\n",
    "        X_list.append(feats)\n",
    "        y_list.append(r[\"language\"])\n",
    "        feature_rows.append({\n",
    "            \"path\": r[\"aug_path\"],\n",
    "            \"variant\": \"aug\",\n",
    "            \"language\": r[\"language\"],\n",
    "            \"gender\": r[\"gender\"]\n",
    "        })\n",
    "\n",
    "X = np.stack(X_list, axis=0)\n",
    "y = np.array(y_list)\n",
    "\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "features_df[\"y\"] = y\n",
    "\n",
    "X.shape, features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a3e96",
   "metadata": {},
   "source": [
    "## 6) Save features for the next notebooks\n",
    "\n",
    "We save:\n",
    "- `X.npy`, `y.npy`\n",
    "- `features.csv` containing metadata for each row in X\n",
    "\n",
    "Next steps (in other notebooks):\n",
    "- Classification.ipynb (train/test split + models)\n",
    "- Clustering.ipynb\n",
    "- Evaluation.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5707fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_shape': (1440, 952),\n",
       " 'num_samples': 1440,\n",
       " 'languages': {'Italian': np.int64(360),\n",
       "  'German': np.int64(360),\n",
       "  'Korean': np.int64(360),\n",
       "  'Spanish': np.int64(360)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(ARTIFACTS_DIR / \"X.npy\", X)\n",
    "np.save(ARTIFACTS_DIR / \"y.npy\", y)\n",
    "\n",
    "features_df.to_csv(ARTIFACTS_DIR / \"features.csv\", index=False)\n",
    "\n",
    "{\n",
    "    \"X_shape\": X.shape,\n",
    "    \"num_samples\": int(len(y)),\n",
    "    \"languages\": dict(pd.Series(y).value_counts())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfe2ac",
   "metadata": {},
   "source": [
    "## 7) Quick sanity checks\n",
    "\n",
    "We check:\n",
    "- class balance\n",
    "- feature NaNs\n",
    "- basic per-language counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e28f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(language  German  Italian  Korean  Spanish\n",
       " variant                                   \n",
       " aug          180      180     180      180\n",
       " clean        180      180     180      180,\n",
       " {'nan_count': 0, 'inf_count': 0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance = features_df.groupby([\"variant\", \"language\"]).size().unstack(fill_value=0)\n",
    "nan_count = int(np.isnan(X).sum())\n",
    "inf_count = int(np.isinf(X).sum())\n",
    "\n",
    "balance, {\"nan_count\": nan_count, \"inf_count\": inf_count}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
